{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/onexixi/DDDGPT/blob/main/Doc2Structure.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📘 Pivot-Banded Structuring: .docx → Trainable Dataset (Colab Prototype)\n",
        "\n",
        "This notebook turns any uploaded **.docx file** into **structured, trainable data** using a geometry-first *pivot + band* algorithm.  \n",
        "It’s inspired by recent breakthroughs in graph algorithms that avoid full sorting and applies the same principle to data curation.\n",
        "\n",
        "---\n",
        "\n",
        "## 🚀 What It Does\n",
        "1. **Upload** a `.docx` (essay, book chapter, research notes, etc.).  \n",
        "2. **Chunking:** Splits the text into ~350-character overlapping chunks.  \n",
        "3. **Embedding:** Encodes chunks into vector space with `all-MiniLM-L6-v2`.  \n",
        "4. **Pivot selection:** Finds diverse “pivot” chunks that represent major regions of the document.  \n",
        "5. **Banding:** Groups all other chunks into distance-based **bands** around pivots.  \n",
        "6. **Metrics:** Computes:\n",
        "   - **Novelty** → how different from pivot.  \n",
        "   - **Redundancy** → how similar to neighbors.  \n",
        "   - **Energy** → novelty × (1 − redundancy).  \n",
        "7. **Outputs structured files:**\n",
        "   - `structured_dataset.jsonl` → all chunks with metadata.  \n",
        "   - `pivots.json` → pivot texts.  \n",
        "   - `train.csv` → simple `(text, label)` pairs for quick prototyping.  \n",
        "   - `dataset_card.md` → dataset description and usage guide.  \n",
        "   - `band_sizes.png` → bar plot of band distribution.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔧 Tweakable Parameters\n",
        "At the top of the code cell you can adjust:\n",
        "- `NUM_PIVOTS` → max number of pivot clusters.  \n",
        "- `TARGET_CH_LEN` → chunk size (characters).  \n",
        "- `KNN` → neighborhood size for redundancy.  \n",
        "- `BAND_QUANTILES` → how many distance bands to use.  \n",
        "\n",
        "---\n",
        "\n",
        "## 🧑‍💻 Example Uses\n",
        "- **Curriculum learning:** Train on band 0 first (closest to pivots), then move outward.  \n",
        "- **Active labeling:** Label only pivots and propagate labels through their bands.  \n",
        "- **Deduplication:** Filter high-redundancy chunks.  \n",
        "- **Energy ranking:** Select high-energy samples for efficient training.  \n",
        "\n",
        "---\n",
        "\n",
        "## 📂 Outputs (after one run)\n",
        "- `structured_dataset.jsonl`  \n",
        "- `pivots.json`  \n",
        "- `train.csv`  \n",
        "- `dataset_card.md`  \n",
        "- `band_sizes.png`  \n",
        "\n",
        "---\n",
        "\n",
        "⚡ **One cell, one upload, structured data ready for training.**"
      ],
      "metadata": {
        "id": "yHztjrJH5Jly"
      },
      "id": "yHztjrJH5Jly"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install python-docx sentence-transformers faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qz9M3Lg4B2h",
        "outputId": "cc311b46-bde1-4485-c105-4ba3ba9ac5ec"
      },
      "id": "6qz9M3Lg4B2h",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e51535d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "5e51535d",
        "outputId": "86aed101-eb79-43f4-9db3-ac49c2ea2f07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload a .docx file…\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-82f137a1-6fa2-4bfe-abbf-9777aa95ba82\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-82f137a1-6fa2-4bfe-abbf-9777aa95ba82\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving A Failed War.docx to A Failed War (1).docx\n",
            "Parsed 16 paragraphs → 50 chunks.\n",
            "Selected 8 pivots.\n",
            "\n",
            "==== Pivots (first 5) ====\n",
            "[pivot 0] chunk#0: A Failed War Written by Mone Levier 40 years ago President Nixon announced a new “War on Drugs” strategy for the United ...\n",
            "[pivot 1] chunk#27: of emergency earlier in the year because of failed efforts. To be sure, International Diplomacy could probably be equate...\n",
            "[pivot 2] chunk#30: e problem but confidence has wained among Americans as well. It is especially important for the U.S. to be able to point...\n",
            "[pivot 3] chunk#34: e the problem is complex, the solution is relatively simple. A Immigration Policy Center Report puts it best in their re...\n",
            "[pivot 4] chunk#37: he U.S. was faced with continual problem of organized crime. The current problem is no different than Italian mob bosses...\n",
            "\n",
            "==== Sample rows (top 10 by energy) ====\n",
            " id  pivot_local_index  band   energy\n",
            " 39                  2     3 0.765108\n",
            " 40                  4     3 0.738753\n",
            " 18                  7     3 0.485184\n",
            " 22                  5     2 0.419905\n",
            " 41                  5     3 0.380202\n",
            "  8                  4     2 0.335889\n",
            " 19                  7     1 0.330972\n",
            " 33                  2     1 0.324072\n",
            " 14                  7     1 0.305185\n",
            " 45                  6     3 0.298768\n",
            "\n",
            "Saved files: structured_dataset.jsonl, pivots.json, train.csv, dataset_card.md, band_sizes.png\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# @title Pivot‑Banded Structuring: .docx → Trainable Dataset (Single Cell, Colab‑Ready)\n",
        "# Copyright (c) 2025\n",
        "# MIT License\n",
        "# ---\n",
        "# Usage:\n",
        "# 1) Run this cell in Google Colab.\n",
        "# 2) Upload a .docx when prompted.\n",
        "# 3) The script will create:\n",
        "#    - structured_dataset.jsonl  (one record per chunk with pivot/band/energy)\n",
        "#    - pivots.json               (list of pivot chunks)\n",
        "#    - train.csv                 (text,label where label is pivot_id)\n",
        "#    - dataset_card.md           (how to use the outputs)\n",
        "#    - band_sizes.png            (simple plot of band counts)\n",
        "# 4) All files will be saved in the current working directory.\n",
        "#\n",
        "# Notes:\n",
        "# - This is a deterministic, geometry‑first prototype inspired by “pivot + bands”\n",
        "#   (SSSP beyond-sorting intuition). It imposes a structured curriculum on a\n",
        "#   completely unstructured document without requiring manual labels.\n",
        "# - You can tune NUM_PIVOTS, KNN, and BAND_QUANTILES below.\n",
        "#\n",
        "# Safe defaults for typical articles/chapters: 1–5k chunks after sentence chunking.\n",
        "# For very large docs, consider raising KNN for smoother neighborhoods.\n",
        "\n",
        "# ======= Install Deps (Colab) =======\n",
        "try:\n",
        "    import google.colab  # type: ignore\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    IN_COLAB = False\n",
        "\n",
        "# Only attempt installs in Colab\n",
        "# if IN_COLAB:\n",
        "#    !pip -q install python-docx sentence-transformers faiss-cpu==1.7.4\n",
        "\n",
        "# ======= Imports =======\n",
        "import os, re, io, math, json, uuid, time, pathlib, textwrap\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "try:\n",
        "    from google.colab import files\n",
        "except Exception:\n",
        "    files = None  # Not in Colab\n",
        "\n",
        "from docx import Document\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# ======= Config =======\n",
        "MODEL_NAME       = \"sentence-transformers/all-MiniLM-L6-v2\"  # small, fast\n",
        "TARGET_CH_LEN    = 350                                      # target chunk size in characters\n",
        "OVERLAP_CH       = 60                                       # overlap to avoid boundary cuts\n",
        "NUM_PIVOTS       = 12                                       # max number of pivots (auto‑capped by data size)\n",
        "KNN              = 16                                       # neighbors for redundancy/coverage\n",
        "BAND_QUANTILES   = [0.25, 0.5, 0.75]                        # define 4 bands per pivot by distance\n",
        "SEED             = 17\n",
        "\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# ======= Helpers =======\n",
        "def read_docx_to_paragraphs(fp: str) -> List[str]:\n",
        "    doc = Document(fp)\n",
        "    paras = [p.text.strip() for p in doc.paragraphs if p.text and p.text.strip()]\n",
        "    return paras\n",
        "\n",
        "def split_into_sentences(text: str) -> List[str]:\n",
        "    # Lightweight sentence splitter (regex). Good enough for prototype.\n",
        "    sents = re.split(r'(?<=[\\.!\\?])\\s+(?=[A-Z0-9(\\\"\\'])', text.strip())\n",
        "    # Fallback if text contains no terminal punctuation\n",
        "    if len(sents) == 1:\n",
        "        sents = re.split(r'\\n+', text.strip())\n",
        "    return [s.strip() for s in sents if s.strip()]\n",
        "\n",
        "def make_chunks(paragraphs: List[str], target_len: int = TARGET_CH_LEN, overlap: int = OVERLAP_CH) -> List[str]:\n",
        "    # Turn paragraphs → sentences → rolling chunks ~ target_len with overlap\n",
        "    chunks = []\n",
        "    buf = \"\"\n",
        "    for para in paragraphs:\n",
        "        for s in split_into_sentences(para):\n",
        "            if len(buf) + 1 + len(s) <= target_len:\n",
        "                buf = (buf + \" \" + s).strip() if buf else s\n",
        "            else:\n",
        "                if buf:\n",
        "                    chunks.append(buf)\n",
        "                # start next with overlap from previous buffer end\n",
        "                if overlap > 0 and len(buf) > overlap:\n",
        "                    carry = buf[-overlap:]\n",
        "                    buf = (carry + \" \" + s).strip()\n",
        "                else:\n",
        "                    buf = s\n",
        "    if buf:\n",
        "        chunks.append(buf)\n",
        "    # de‑dup tiny stragglers\n",
        "    chunks = [c.strip() for c in chunks if len(c.strip()) >= 20]\n",
        "    return chunks\n",
        "\n",
        "def embed_texts(texts: List[str], model_name: str = MODEL_NAME, batch_size: int = 64) -> np.ndarray:\n",
        "    model = SentenceTransformer(model_name)\n",
        "    vecs = model.encode(texts, batch_size=batch_size, convert_to_numpy=True, show_progress_bar=len(texts) > 128, normalize_embeddings=True)\n",
        "    return vecs.astype(np.float32)\n",
        "\n",
        "def build_faiss_index(vecs: np.ndarray) -> faiss.Index:\n",
        "    d = vecs.shape[1]\n",
        "    index = faiss.IndexFlatIP(d)  # cosine sim since normalized\n",
        "    index.add(vecs)\n",
        "    return index\n",
        "\n",
        "def greedy_farthest_pivots(vecs: np.ndarray, max_pivots: int) -> List[int]:\n",
        "    # Farthest‑point sampling in cosine distance (1 - sim)\n",
        "    n = vecs.shape[0]\n",
        "    if n == 0:\n",
        "        return []\n",
        "    max_pivots = min(max_pivots, max(1, n))\n",
        "    # start with the most \"average‑distant\" point\n",
        "    centroid = vecs.mean(axis=0, keepdims=True)\n",
        "    sims = (vecs @ centroid.T).ravel()\n",
        "    seed = int(np.argmin(sims))  # least similar to centroid\n",
        "    pivots = [seed]\n",
        "    # distance to nearest pivot for each point\n",
        "    nearest_sim = vecs @ vecs[seed]\n",
        "    for _ in range(1, max_pivots):\n",
        "        # pick the point with minimum similarity to any pivot (i.e., farthest)\n",
        "        pick = int(np.argmin(nearest_sim))\n",
        "        if pick in pivots:\n",
        "            # if repetition, choose a random remaining\n",
        "            remaining = [i for i in range(n) if i not in pivots]\n",
        "            if not remaining:\n",
        "                break\n",
        "            pick = int(np.random.choice(remaining))\n",
        "        pivots.append(pick)\n",
        "        # update nearest_sim\n",
        "        cand = vecs @ vecs[pick]\n",
        "        nearest_sim = np.maximum(nearest_sim, cand)\n",
        "    return sorted(list(set(pivots)))\n",
        "\n",
        "def assign_to_nearest_pivot(vecs: np.ndarray, pivots: List[int]) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    # Returns (assignments, distances) where distance = 1 - cosine_sim_to_pivot\n",
        "    if len(pivots) == 0:\n",
        "        return np.array([], dtype=int), np.array([], dtype=np.float32)\n",
        "    P = vecs[pivots]  # (p, d)\n",
        "    sims = vecs @ P.T  # (n, p)\n",
        "    best = sims.argmax(axis=1)\n",
        "    best_sim = sims[np.arange(vecs.shape[0]), best]\n",
        "    dist = 1.0 - best_sim\n",
        "    return best.astype(int), dist.astype(np.float32)\n",
        "\n",
        "def compute_bands_for_pivot(dists: np.ndarray, quantiles: List[float]) -> np.ndarray:\n",
        "    # Given distances for items assigned to a single pivot, return per‑item band in {0..len(quantiles)}\n",
        "    if dists.size == 0:\n",
        "        return np.array([], dtype=int)\n",
        "    qs = np.quantile(dists, quantiles).tolist()\n",
        "    def band_of(x):\n",
        "        for b, q in enumerate(qs):\n",
        "            if x <= q: return b\n",
        "        return len(qs)\n",
        "    return np.array([band_of(x) for x in dists], dtype=int)\n",
        "\n",
        "def knn_redundancy(vecs: np.ndarray, k: int = KNN) -> np.ndarray:\n",
        "    # Mean similarity to k nearest neighbors (excluding self). Higher = more redundant.\n",
        "    idx = build_faiss_index(vecs)\n",
        "    k_eff = min(k+1, vecs.shape[0])\n",
        "    sims, inds = idx.search(vecs, k_eff)  # includes self at rank 0\n",
        "    sims = sims[:, 1:] if sims.shape[1] > 1 else sims  # drop self\n",
        "    if sims.size == 0:\n",
        "        return np.zeros(vecs.shape[0], dtype=np.float32)\n",
        "    # sims are dot products (cosine), average them\n",
        "    return sims.mean(axis=1).astype(np.float32)\n",
        "\n",
        "def minmax_norm(x: np.ndarray) -> np.ndarray:\n",
        "    if x.size == 0:\n",
        "        return x\n",
        "    lo, hi = float(np.min(x)), float(np.max(x))\n",
        "    if hi - lo < 1e-8:\n",
        "        return np.zeros_like(x)\n",
        "    return (x - lo) / (hi - lo)\n",
        "\n",
        "# ======= Main Pipeline =======\n",
        "def main():\n",
        "    # --- Upload .docx (Colab) or read local file variable DOCX_PATH\n",
        "    if IN_COLAB and files is not None:\n",
        "        print(\"Please upload a .docx file…\")\n",
        "        up = files.upload()\n",
        "        if not up:\n",
        "            raise RuntimeError(\"No file uploaded.\")\n",
        "        docx_name = list(up.keys())[0]\n",
        "        docx_path = docx_name\n",
        "    else:\n",
        "        # Not in Colab: set DOCX_PATH manually if running locally\n",
        "        docx_path = os.environ.get(\"DOCX_PATH\", \"\").strip()\n",
        "        if not docx_path or not os.path.exists(docx_path):\n",
        "            raise RuntimeError(\"Set DOCX_PATH env var to a .docx file path when not in Colab.\")\n",
        "\n",
        "    # --- Parse and chunk\n",
        "    paragraphs = read_docx_to_paragraphs(docx_path)\n",
        "    if not paragraphs:\n",
        "        raise RuntimeError(\"No paragraphs found. Is the .docx valid/non‑empty?\")\n",
        "    chunks = make_chunks(paragraphs, TARGET_CH_LEN, OVERLAP_CH)\n",
        "    print(f\"Parsed {len(paragraphs)} paragraphs → {len(chunks)} chunks.\")\n",
        "\n",
        "    # --- Embed\n",
        "    vecs = embed_texts(chunks, MODEL_NAME)\n",
        "    n = vecs.shape[0]\n",
        "\n",
        "    # --- Pivots via farthest‑point sampling\n",
        "    pivots = greedy_farthest_pivots(vecs, max_pivots=min(NUM_PIVOTS, max(1, int(np.ceil(np.sqrt(n))))))\n",
        "    print(f\"Selected {len(pivots)} pivots.\")\n",
        "\n",
        "    # --- Assign each chunk to nearest pivot; compute novelty (distance to pivot)\n",
        "    assign_idx, novelty_dist = assign_to_nearest_pivot(vecs, pivots)\n",
        "\n",
        "    # --- Redundancy via mean similarity to K nearest neighbors\n",
        "    redundancy_sim = knn_redundancy(vecs, KNN)\n",
        "\n",
        "    # Normalize novelty (distance) high→novel, and redundancy high→redundant\n",
        "    novelty_norm    = minmax_norm(novelty_dist)  # 0..1\n",
        "    redundancy_norm = minmax_norm(redundancy_sim)  # 0..1\n",
        "\n",
        "    # Energy: novelty * (1 - redundancy)\n",
        "    energy = (novelty_norm * (1.0 - redundancy_norm)).astype(np.float32)\n",
        "\n",
        "    # --- Build bands per pivot\n",
        "    band_ids = np.zeros(n, dtype=int)\n",
        "    for p_local, p in enumerate(pivots):\n",
        "        mask = (assign_idx == p_local)\n",
        "        pdists = novelty_dist[mask]\n",
        "        pbands = compute_bands_for_pivot(pdists, BAND_QUANTILES)\n",
        "        band_ids[mask] = pbands\n",
        "\n",
        "    # --- Prepare outputs\n",
        "    pivot_texts = [chunks[i] for i in pivots]\n",
        "    records = []\n",
        "    for i, text in enumerate(chunks):\n",
        "        pid_local = int(assign_idx[i])\n",
        "        pid_global = int(pivots[pid_local])\n",
        "        rec = {\n",
        "            \"id\": i,\n",
        "            \"text\": text,\n",
        "            \"pivot_id\": pid_global,\n",
        "            \"pivot_local_index\": pid_local,\n",
        "            \"pivot_text\": chunks[pid_global],\n",
        "            \"band\": int(band_ids[i]),\n",
        "            \"novelty\": float(novelty_norm[i]),\n",
        "            \"redundancy\": float(redundancy_norm[i]),\n",
        "            \"energy\": float(energy[i]),\n",
        "            \"source\": os.path.basename(docx_path),\n",
        "        }\n",
        "        records.append(rec)\n",
        "\n",
        "    # DataFrames\n",
        "    df = pd.DataFrame(records).sort_values([\"pivot_local_index\", \"band\", \"energy\"], ascending=[True, True, False])\n",
        "    piv_df = pd.DataFrame({\"pivot_id\": pivots, \"pivot_text\": pivot_texts})\n",
        "\n",
        "    # --- Save files\n",
        "    out_jsonl = \"structured_dataset.jsonl\"\n",
        "    out_piv   = \"pivots.json\"\n",
        "    out_csv   = \"train.csv\"\n",
        "    out_card  = \"dataset_card.md\"\n",
        "\n",
        "    with open(out_jsonl, \"w\", encoding=\"utf-8\") as f:\n",
        "        for rec in records:\n",
        "            f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "    with open(out_piv, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump({\"pivots\": [{\"pivot_id\": int(i), \"text\": t} for i, t in zip(pivots, pivot_texts)]}, f, ensure_ascii=False, indent=2)\n",
        "    df_csv = df[[\"text\", \"pivot_local_index\"]].rename(columns={\"pivot_local_index\": \"label\"})\n",
        "    df_csv.to_csv(out_csv, index=False)\n",
        "\n",
        "    with open(out_card, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(textwrap.dedent(f'''\n",
        "        # Dataset Card — Pivot‑Banded Structuring (Prototype)\n",
        "\n",
        "        **Source doc:** `{os.path.basename(docx_path)}`\n",
        "        **Chunks:** {n}\n",
        "        **Pivots:** {len(pivots)}\n",
        "        **Bands:** {len(BAND_QUANTILES)+1} per pivot\n",
        "\n",
        "        ## Files\n",
        "        - `structured_dataset.jsonl`: One JSON object per chunk with fields:\n",
        "          - `id`, `text`\n",
        "          - `pivot_id`, `pivot_local_index`, `pivot_text`\n",
        "          - `band` (0 = closest to pivot)\n",
        "          - `novelty` in [0,1], `redundancy` in [0,1], `energy` = novelty * (1 - redundancy)\n",
        "          - `source`\n",
        "        - `pivots.json`: Pivot list with their texts.\n",
        "        - `train.csv`: Simple (text, label) pairs where `label` is `pivot_local_index` for quick prototyping.\n",
        "\n",
        "        ## Suggested Uses\n",
        "        - **Curriculum training:** sample band 0 first, then 1..k\n",
        "        - **Active labeling:** label only `pivots` and propagate to their bands\n",
        "        - **Dedup:** filter chunks with high `redundancy`\n",
        "        - **High‑value mining:** prioritize top `energy` chunks per pivot\n",
        "\n",
        "        ## Notes\n",
        "        - Embeddings: `{MODEL_NAME}` normalized for cosine similarity.\n",
        "        - Neighborhoods use FAISS (inner product) with K={KNN}.\n",
        "        - Pivots via greedy farthest‑point sampling.\n",
        "        - Bands by distance quantiles: {BAND_QUANTILES + [1.0]}.\n",
        "\n",
        "        ## Repro / Tuning\n",
        "        - Adjust `NUM_PIVOTS`, `TARGET_CH_LEN`, `KNN`, and `BAND_QUANTILES` at the top.\n",
        "        - For very large docs, consider IVF/PQ FAISS indexes (not included here).\n",
        "        ''').strip())\n",
        "\n",
        "    # --- Simple plot: band sizes (matplotlib, single plot, default colors)\n",
        "    band_counts = df.groupby(\"band\").size().sort_index()\n",
        "    plt.figure()\n",
        "    band_counts.plot(kind=\"bar\")\n",
        "    plt.title(\"Band Sizes\")\n",
        "    plt.xlabel(\"Band\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"band_sizes.png\", dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "    # --- Preview\n",
        "    print(\"\\n==== Pivots (first 5) ====\")\n",
        "    for i, (pid, ptxt) in enumerate(zip(pivots[:5], pivot_texts[:5])):\n",
        "        print(f\"[pivot {i}] chunk#{pid}: {ptxt[:120].replace('\\n',' ')}{'...' if len(ptxt)>120 else ''}\")\n",
        "    print(\"\\n==== Sample rows (top 10 by energy) ====\")\n",
        "    disp = df.sort_values(\"energy\", ascending=False).head(10)\n",
        "    print(disp[[\"id\",\"pivot_local_index\",\"band\",\"energy\"]].to_string(index=False))\n",
        "    print(\"\\nSaved files: structured_dataset.jsonl, pivots.json, train.csv, dataset_card.md, band_sizes.png\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}